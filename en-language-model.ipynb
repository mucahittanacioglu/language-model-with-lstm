{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./data/war&peace.txt\",\"r\",encoding=\"utf-8\")\n",
    "data=file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# considering punctuations as word\n",
    "import string\n",
    "punct = \".,!?/’:\\\";\\-–“”'\"\n",
    "data = data.lower()\n",
    "data = data.replace(\"\\n\",\" \")\n",
    "tmp = \"\"\n",
    "for i in data:\n",
    "    if i.isalpha() or i == \" \":\n",
    "        tmp = tmp+i\n",
    "    elif i in punct:\n",
    "        tmp = tmp+\" \"+i+\" \"\n",
    "data = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 50000  # Number of words \n",
    "n_word = 5 # Number of word as input to model\n",
    "embedding_dim = 50 # Embedding vector dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_features,lower=False,filters=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts([data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded = tokenizer.texts_to_sequences([data])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabluary size: 19263\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index)+1\n",
    "print(f\"Vocabluary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Input&Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for i in range(n_word, len(data_encoded)):\n",
    "    X.append(data_encoded[i-n_word:i])\n",
    "    y.append(data_encoded[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "X = np.array(X)\n",
    "#y = to_categorical(y, num_classes=vocab_size)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained GloVe embedding 6B tokens, 400K vocab size  50d vectors\n",
    "# source https://nlp.stanford.edu/projects/glove/\n",
    "embedding = open(\"embeddings/glove.6B.50d.txt\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating embedding dictionary\n",
    "embedding_dict= {}\n",
    "for i in embedding:\n",
    "    line = i.split(' ')\n",
    "    line[-1]=line[-1].replace('\\n','')\n",
    "    embedding_dict[''+line[0]]=line[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = embedding_dict.keys()\n",
    "unk_token = np.zeros(embedding_dim,)\n",
    "#Using unk_token(average of all vetors) for the words not in embedding ectors\n",
    "for i in keys:\n",
    "    unk_token = unk_token + np.array(embedding_dict[i],float)\n",
    "unk_token = unk_token / len(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating embedding matrix\n",
    "counter = 0\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "embedding_matrix[0] = np.zeros(embedding_dim,)\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i == vocab_size:\n",
    "        break\n",
    "    embedding_vector = embedding_dict.get(word)\n",
    "    if embedding_vector is  None:\n",
    "        #words not in embedding considered unk token\n",
    "        embedding_vector = unk_token.tolist()\n",
    "        counter = counter + 1\n",
    "    embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19263, 50)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unk token usage: 3339\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of unk token usage: {counter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 5, 50)             963150    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 5, 128)            91648     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 19263)             1252095   \n",
      "=================================================================\n",
      "Total params: 2,372,877\n",
      "Trainable params: 1,409,727\n",
      "Non-trainable params: 963,150\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,RNN,Dense,Embedding,Input,Flatten\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(5,)))\n",
    "model.add(Embedding(input_dim = vocab_size , weights =[embedding_matrix], output_dim = embedding_dim, input_length = n_word, trainable=False))\n",
    "model.add(LSTM(128,return_sequences = True))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(128,activation = \"relu\"))\n",
    "model.add(Dense(64,activation = \"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile&Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call back for stop training at certein accuracy threshold\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "class My_Callback(Callback):\n",
    "    def __init__(self, threshold,model_save_cp):\n",
    "        super(My_Callback, self).__init__()\n",
    "        self.threshold = threshold\n",
    "        self.model_save_cp = model_save_cp\n",
    "    def on_epoch_end(self, epoch, logs=None): \n",
    "        acc = logs[\"sparse_categorical_accuracy\"]\n",
    "        if (epoch+1) % self.model_save_cp == 0:\n",
    "            print(f\"\\nModel Checkpoint reached saving model weights...\\n\")\n",
    "            self.model.save_weights(f\"model/en-lang-model-{epoch+1}ep.h5\")\n",
    "        if acc >= self.threshold:\n",
    "            print(f\"Accuracy reach over {self.threshold}% terminating train process.\")\n",
    "            self.model.save_weights(f\"model/en-lang-model-{epoch}ep.h5\")\n",
    "            self.model.stop_training = True\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "166/166 [==============================] - 21s 103ms/step - loss: 7.6199 - sparse_categorical_accuracy: 0.0372\n",
      "Epoch 2/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 6.3274 - sparse_categorical_accuracy: 0.0589\n",
      "Epoch 3/1000\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 6.2258 - sparse_categorical_accuracy: 0.0719\n",
      "Epoch 4/1000\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 6.0853 - sparse_categorical_accuracy: 0.0840\n",
      "Epoch 5/1000\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 5.9245 - sparse_categorical_accuracy: 0.0949\n",
      "Epoch 6/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 5.7276 - sparse_categorical_accuracy: 0.1111\n",
      "Epoch 7/1000\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 5.5821 - sparse_categorical_accuracy: 0.1244\n",
      "Epoch 8/1000\n",
      "166/166 [==============================] - 17s 104ms/step - loss: 5.4703 - sparse_categorical_accuracy: 0.1358\n",
      "Epoch 9/1000\n",
      "166/166 [==============================] - 18s 109ms/step - loss: 5.3885 - sparse_categorical_accuracy: 0.1418\n",
      "Epoch 10/1000\n",
      "166/166 [==============================] - 18s 107ms/step - loss: 5.3275 - sparse_categorical_accuracy: 0.1470\n",
      "Epoch 11/1000\n",
      "166/166 [==============================] - 18s 107ms/step - loss: 5.2683 - sparse_categorical_accuracy: 0.1515\n",
      "Epoch 12/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 5.2195 - sparse_categorical_accuracy: 0.1545\n",
      "Epoch 13/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 5.1729 - sparse_categorical_accuracy: 0.1575\n",
      "Epoch 14/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 5.1290 - sparse_categorical_accuracy: 0.1603\n",
      "Epoch 15/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 5.0939 - sparse_categorical_accuracy: 0.1617\n",
      "Epoch 16/1000\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 5.0495 - sparse_categorical_accuracy: 0.1632\n",
      "Epoch 17/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 5.0276 - sparse_categorical_accuracy: 0.1650\n",
      "Epoch 18/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 4.9858 - sparse_categorical_accuracy: 0.1672\n",
      "Epoch 19/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 4.9586 - sparse_categorical_accuracy: 0.1687\n",
      "Epoch 20/1000\n",
      "166/166 [==============================] - 17s 101ms/step - loss: 4.9287 - sparse_categorical_accuracy: 0.1694\n",
      "Epoch 21/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 4.9006 - sparse_categorical_accuracy: 0.1712\n",
      "Epoch 22/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 4.8782 - sparse_categorical_accuracy: 0.1719\n",
      "Epoch 23/1000\n",
      "166/166 [==============================] - 18s 111ms/step - loss: 4.8544 - sparse_categorical_accuracy: 0.1728\n",
      "Epoch 24/1000\n",
      "166/166 [==============================] - 19s 115ms/step - loss: 4.8318 - sparse_categorical_accuracy: 0.1736\n",
      "Epoch 25/1000\n",
      "166/166 [==============================] - 19s 113ms/step - loss: 4.8114 - sparse_categorical_accuracy: 0.1750\n",
      "Epoch 26/1000\n",
      "166/166 [==============================] - 18s 107ms/step - loss: 4.7898 - sparse_categorical_accuracy: 0.1765\n",
      "Epoch 27/1000\n",
      "166/166 [==============================] - 17s 104ms/step - loss: 4.7710 - sparse_categorical_accuracy: 0.1764\n",
      "Epoch 28/1000\n",
      "166/166 [==============================] - 18s 106ms/step - loss: 4.7485 - sparse_categorical_accuracy: 0.1786\n",
      "Epoch 29/1000\n",
      "166/166 [==============================] - 18s 110ms/step - loss: 4.7291 - sparse_categorical_accuracy: 0.1791\n",
      "Epoch 30/1000\n",
      "166/166 [==============================] - 18s 108ms/step - loss: 4.7115 - sparse_categorical_accuracy: 0.1804\n",
      "Epoch 31/1000\n",
      "166/166 [==============================] - 18s 107ms/step - loss: 4.6959 - sparse_categorical_accuracy: 0.1809\n",
      "Epoch 32/1000\n",
      "166/166 [==============================] - 18s 107ms/step - loss: 4.6735 - sparse_categorical_accuracy: 0.1823\n",
      "Epoch 33/1000\n",
      "166/166 [==============================] - 17s 105ms/step - loss: 4.6637 - sparse_categorical_accuracy: 0.1819\n",
      "Epoch 34/1000\n",
      "166/166 [==============================] - 18s 106ms/step - loss: 4.6477 - sparse_categorical_accuracy: 0.1827\n",
      "Epoch 35/1000\n",
      "166/166 [==============================] - 17s 105ms/step - loss: 4.6303 - sparse_categorical_accuracy: 0.1840\n",
      "Epoch 36/1000\n",
      "166/166 [==============================] - 18s 107ms/step - loss: 4.6152 - sparse_categorical_accuracy: 0.1849\n",
      "Epoch 37/1000\n",
      "166/166 [==============================] - 18s 109ms/step - loss: 4.5943 - sparse_categorical_accuracy: 0.1856\n",
      "Epoch 38/1000\n",
      "166/166 [==============================] - 17s 105ms/step - loss: 4.5808 - sparse_categorical_accuracy: 0.1868\n",
      "Epoch 39/1000\n",
      "166/166 [==============================] - 17s 105ms/step - loss: 4.5741 - sparse_categorical_accuracy: 0.1864\n",
      "Epoch 40/1000\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 4.5472 - sparse_categorical_accuracy: 0.1885\n",
      "Epoch 41/1000\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 4.5469 - sparse_categorical_accuracy: 0.1890\n",
      "Epoch 42/1000\n",
      "166/166 [==============================] - 18s 107ms/step - loss: 4.5317 - sparse_categorical_accuracy: 0.1897\n",
      "Epoch 43/1000\n",
      "166/166 [==============================] - 17s 105ms/step - loss: 4.5155 - sparse_categorical_accuracy: 0.1905\n",
      "Epoch 44/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 4.5011 - sparse_categorical_accuracy: 0.1915\n",
      "Epoch 45/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 4.4884 - sparse_categorical_accuracy: 0.1918\n",
      "Epoch 46/1000\n",
      "166/166 [==============================] - 18s 108ms/step - loss: 4.4809 - sparse_categorical_accuracy: 0.1916\n",
      "Epoch 47/1000\n",
      "166/166 [==============================] - 17s 104ms/step - loss: 4.4708 - sparse_categorical_accuracy: 0.1929\n",
      "Epoch 48/1000\n",
      "166/166 [==============================] - 17s 105ms/step - loss: 4.4612 - sparse_categorical_accuracy: 0.1924\n",
      "Epoch 49/1000\n",
      "166/166 [==============================] - 18s 106ms/step - loss: 4.4521 - sparse_categorical_accuracy: 0.1943\n",
      "Epoch 50/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 4.4397 - sparse_categorical_accuracy: 0.1952\n",
      "\n",
      "Model Checkpoint reached saving model weights...\n",
      "\n",
      "Epoch 51/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 4.4295 - sparse_categorical_accuracy: 0.1963\n",
      "Epoch 52/1000\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 4.4218 - sparse_categorical_accuracy: 0.1962\n",
      "Epoch 53/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 4.4092 - sparse_categorical_accuracy: 0.1972\n",
      "Epoch 54/1000\n",
      "166/166 [==============================] - 18s 106ms/step - loss: 4.3956 - sparse_categorical_accuracy: 0.1979\n",
      "Epoch 55/1000\n",
      "166/166 [==============================] - 19s 112ms/step - loss: 4.3909 - sparse_categorical_accuracy: 0.1981\n",
      "Epoch 56/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 4.3822 - sparse_categorical_accuracy: 0.1987\n",
      "Epoch 57/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 4.3730 - sparse_categorical_accuracy: 0.2002\n",
      "Epoch 58/1000\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 4.3696 - sparse_categorical_accuracy: 0.1994\n",
      "Epoch 59/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 4.3557 - sparse_categorical_accuracy: 0.2015\n",
      "Epoch 60/1000\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 4.3463 - sparse_categorical_accuracy: 0.2023\n",
      "Epoch 61/1000\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 4.3440 - sparse_categorical_accuracy: 0.2015\n",
      "Epoch 62/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 4.3323 - sparse_categorical_accuracy: 0.2029\n",
      "Epoch 63/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 4.3224 - sparse_categorical_accuracy: 0.2036\n",
      "Epoch 64/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 4.3222 - sparse_categorical_accuracy: 0.2029\n",
      "Epoch 65/1000\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 4.3116 - sparse_categorical_accuracy: 0.2042\n",
      "Epoch 66/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166/166 [==============================] - 17s 102ms/step - loss: 4.3077 - sparse_categorical_accuracy: 0.2045\n",
      "Epoch 67/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 4.3009 - sparse_categorical_accuracy: 0.2051\n",
      "Epoch 68/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 4.2929 - sparse_categorical_accuracy: 0.2060\n",
      "Epoch 69/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 4.2827 - sparse_categorical_accuracy: 0.2067\n",
      "Epoch 70/1000\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 4.2762 - sparse_categorical_accuracy: 0.2084\n",
      "Epoch 71/1000\n",
      "166/166 [==============================] - 17s 104ms/step - loss: 4.2701 - sparse_categorical_accuracy: 0.2087\n",
      "Epoch 72/1000\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 4.2719 - sparse_categorical_accuracy: 0.2086\n",
      "Epoch 73/1000\n",
      "166/166 [==============================] - 18s 107ms/step - loss: 4.2593 - sparse_categorical_accuracy: 0.2099\n",
      "Epoch 74/1000\n",
      "166/166 [==============================] - 27s 161ms/step - loss: 4.2611 - sparse_categorical_accuracy: 0.2084\n",
      "Epoch 75/1000\n",
      "166/166 [==============================] - 33s 198ms/step - loss: 4.2496 - sparse_categorical_accuracy: 0.2095\n",
      "Epoch 76/1000\n",
      "166/166 [==============================] - 19s 116ms/step - loss: 4.2410 - sparse_categorical_accuracy: 0.2111\n",
      "Epoch 77/1000\n",
      "166/166 [==============================] - 17s 104ms/step - loss: 4.2379 - sparse_categorical_accuracy: 0.2116\n",
      "Epoch 78/1000\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 4.2285 - sparse_categorical_accuracy: 0.2119\n",
      "Epoch 79/1000\n",
      "166/166 [==============================] - 17s 104ms/step - loss: 4.2268 - sparse_categorical_accuracy: 0.2128\n",
      "Epoch 80/1000\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 4.2284 - sparse_categorical_accuracy: 0.2123\n",
      "Epoch 81/1000\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 4.2197 - sparse_categorical_accuracy: 0.2125\n",
      "Epoch 82/1000\n",
      "166/166 [==============================] - 17s 104ms/step - loss: 4.2162 - sparse_categorical_accuracy: 0.2131\n",
      "Epoch 83/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 4.2103 - sparse_categorical_accuracy: 0.2138\n",
      "Epoch 84/1000\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 4.2056 - sparse_categorical_accuracy: 0.2143\n",
      "Epoch 85/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 4.2007 - sparse_categorical_accuracy: 0.2143\n",
      "Epoch 86/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 4.1963 - sparse_categorical_accuracy: 0.2152\n",
      "Epoch 87/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 4.1898 - sparse_categorical_accuracy: 0.2152\n",
      "Epoch 88/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 4.1855 - sparse_categorical_accuracy: 0.2159\n",
      "Epoch 89/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 4.1841 - sparse_categorical_accuracy: 0.2161\n",
      "Epoch 90/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 4.1774 - sparse_categorical_accuracy: 0.2168\n",
      "Epoch 91/1000\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 4.1725 - sparse_categorical_accuracy: 0.2177\n",
      "Epoch 92/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 4.1648 - sparse_categorical_accuracy: 0.2179\n",
      "Epoch 93/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 4.1655 - sparse_categorical_accuracy: 0.2181\n",
      "Epoch 94/1000\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 4.1649 - sparse_categorical_accuracy: 0.2181\n",
      "Epoch 95/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 4.1557 - sparse_categorical_accuracy: 0.2182\n",
      "Epoch 96/1000\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 4.1524 - sparse_categorical_accuracy: 0.2191\n",
      "Epoch 97/1000\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 4.1472 - sparse_categorical_accuracy: 0.2200\n",
      "Epoch 98/1000\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 4.1456 - sparse_categorical_accuracy: 0.2203\n",
      "Epoch 99/1000\n",
      "166/166 [==============================] - 17s 104ms/step - loss: 4.1396 - sparse_categorical_accuracy: 0.2201\n",
      "Epoch 100/1000\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 4.1408 - sparse_categorical_accuracy: 0.2206\n",
      "\n",
      "Model Checkpoint reached saving model weights...\n",
      "\n",
      "Epoch 101/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 4.1263 - sparse_categorical_accuracy: 0.2216\n",
      "Epoch 102/1000\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 4.1333 - sparse_categorical_accuracy: 0.2206\n",
      "Epoch 103/1000\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 4.1181 - sparse_categorical_accuracy: 0.2230\n",
      "Epoch 104/1000\n",
      "166/166 [==============================] - 17s 104ms/step - loss: 4.1232 - sparse_categorical_accuracy: 0.2224\n",
      "Epoch 105/1000\n",
      "166/166 [==============================] - 17s 103ms/step - loss: 4.1192 - sparse_categorical_accuracy: 0.2224\n",
      "Epoch 106/1000\n",
      "166/166 [==============================] - 17s 102ms/step - loss: 4.1146 - sparse_categorical_accuracy: 0.2225\n",
      "Epoch 107/1000\n",
      "135/166 [=======================>......] - ETA: 3s - loss: 4.1086 - sparse_categorical_accuracy: 0.2239"
     ]
    }
   ],
   "source": [
    "#categorical_accuracy\n",
    "cb = My_Callback(threshold = 0.95, model_save_cp = 50)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['sparse_categorical_accuracy'])\n",
    "history = model.fit(X, y, batch_size=4096, epochs=1000, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights(\"model/en-lang-model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "def plot_history(history):\n",
    "    acc = history.history['categorical_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.title('Training  accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'r', label='Training loss')\n",
    "    plt.title('Training loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate sequence basically get seed text and and consider 5 word from right side and ignore others, if its less than 5 padding(pre) from left side after that for each 5 word model predict 1 word. After prediction we feed 4 word from previous input and previously predicted word as 5th word and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dynamic printing\n",
    "from IPython.display import display, clear_output\n",
    "# padding for fixed input size\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# generating sequence from language model\n",
    "def generate_sequence(tokenizer, max_output_seq_len, n_input_words, seed_text, model):\n",
    "    model_input = seed_text\n",
    "    \n",
    "    # generate given max amount of word\n",
    "    for _ in range(max_output_seq_len):\n",
    "        # tokenize input text\n",
    "        model_input_encoded = tokenizer.texts_to_sequences([model_input])[0]\n",
    "        # pre-padding for fixed length(n_input_words)\n",
    "        model_input_encoded = pad_sequences([model_input_encoded], maxlen=n_input_words, padding='pre')\n",
    "        # predict probabilties for each word in vocab.\n",
    "        prediction = model.predict(model_input_encoded)[0]\n",
    "        prediction = np.argmax(prediction)\n",
    "        # finding predicted word\n",
    "        predicted_word = list(tokenizer.word_index.keys())[prediction-1]\n",
    "        model_input = model_input + \" \" + predicted_word\n",
    "        display(model_input)\n",
    "        clear_output(wait=True)\n",
    "    return model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dynamic printing\n",
    "from IPython.display import display, clear_output\n",
    "# padding for fixed input size\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# generating sequence from language model\n",
    "def generate_sequence_with_random_choice(tokenizer, max_output_seq_len, n_input_words, seed_text, model, acceptance_threshold):\n",
    "    model_input = seed_text\n",
    "    \n",
    "    # generate given max amount of word\n",
    "    for _ in range(max_output_seq_len):\n",
    "        # variable countermeasure if threshold too high for prediction\n",
    "        at = acceptance_threshold\n",
    "        # tokenize input text\n",
    "        model_input_encoded = tokenizer.texts_to_sequences([model_input])[0]\n",
    "        # pre-padding for fixed length(n_input_words)\n",
    "        model_input_encoded = pad_sequences([model_input_encoded], maxlen=n_input_words, padding='pre')\n",
    "        # predict probabilties for each word in vocab.\n",
    "        prediction = model.predict(model_input_encoded)\n",
    "        # countermeasure if threshold too high for prediction\n",
    "        if np.max(prediction) < acceptance_threshold:\n",
    "            at = np.max(prediction)\n",
    "        # take predicted words with probability higher than the threshold\n",
    "        prediction = prediction >= at\n",
    "        # taking index high prob. words(true values)\n",
    "        possible_words = np.where(prediction)[1]\n",
    "        # choosing one of possible word\n",
    "        choosen_index = possible_words[np.random.randint(0,len(possible_words))]\n",
    "        # getting correspanding key from word index\n",
    "        predicted_word = list(tokenizer.word_index.keys())[choosen_index-1]\n",
    "        model_input = model_input + \" \" + predicted_word\n",
    "        display(model_input)\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "    return model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sequence(tokenizer, 25, n_word, \"Nicholas looked the sky and\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sequence_with_random_choice(tokenizer, 25, n_word, \"Nicholas looked the sky and\", model,acceptance_threshold = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sequence(tokenizer, 150, n_word, \"She suddenly\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sequence_with_random_choice(tokenizer, 150, n_word, \"She suddenly \", model,acceptance_threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sequence(tokenizer, 50, n_word, \"The Italian seemed happy\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
